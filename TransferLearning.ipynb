{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this work I will try to classify X-ray images, taken from\n",
        "the dataset of Chest X-ray images, to Pneumonia/Normal.\n",
        "Dataset is shared on https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
        "Dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal).\n",
        "There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "These two major transfer learning scenarios look as follows:\n",
        "\n",
        "-  **Finetuning the convnet**: Instead of random initializaion, we\n",
        "   initialize the network with a pretrained network, like the one that is\n",
        "   trained on imagenet 1000 dataset. Rest of the training looks as\n",
        "   usual.\n",
        "-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n",
        "   for all of the network except that of the final fully connected\n",
        "   layer or layers. These last fully connected layers are replaced with a new layer or layers\n",
        "   with random weights and only these layers are trained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xufhO5P1doS5",
        "outputId": "d1ff0e66-3358-48b6-bbc2-2f1d4d7801ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from  google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uk6wcA1ejH4",
        "outputId": "8569501a-9aff-444d-85ff-1f3721e5b227"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['train', 'val', 'test']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.listdir('/content/drive/MyDrive/datasets/chest_xray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjpNDp-xfXF0",
        "outputId": "0b74af16-9f5f-4099-8671-8ae0ff4c479b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
        "from torch.autograd import Variable\n",
        "import itertools\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icCm_HpSdl-1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvif6hHHey2C"
      },
      "outputs": [],
      "source": [
        "# --- General Hyperparameters ---\n",
        "num_epochs = 10\n",
        "max_epochs_stop = 10\n",
        "batch_size = 4\n",
        "num_workers = 4\n",
        "\n",
        "# --- Fine-tuning Model Parameters ---\n",
        "finetune_learning_rate = 1e-4\n",
        "finetune_model_name = 'FineTuned_model.pth'\n",
        "finetune_scheduler_step_size = 7\n",
        "finetune_scheduler_gamma = 0.1\n",
        "\n",
        "# --- Fixed Feature Extractor Model Parameters ---\n",
        "fixed_extractor_learning_rate = 0.001\n",
        "fixed_extractor_momentum = 0.9\n",
        "fixed_extractor_model_name = 'Pretrained_model.pth'\n",
        "fixed_extractor_scheduler_step_size = 7\n",
        "fixed_extractor_scheduler_gamma = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vm_q8hufqwt",
        "outputId": "949844d1-7ab1-4c33-c680-17fd77ba6868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes: {'train': 5216, 'val': 16, 'test': 624}\n",
            "Class names: ['NORMAL', 'PNEUMONIA']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation and normalization\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(256),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Define path to the data directory\n",
        "root_dir ='/content/drive/MyDrive/models'\n",
        "data_dir = os.path.join(os.path.join(root_dir, 'data'), 'chest_xray')\n",
        "data_dir='/content/drive/MyDrive/datasets/chest_xray'\n",
        "# Load datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in ['train', 'val', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(\"Dataset sizes:\", dataset_sizes)\n",
        "print(\"Class names:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--EHW9m4f7hl"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs, max_epochs_stop, model_name):\n",
        "    since = time.time()\n",
        "    best_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    loss_train, acc_train = [], []\n",
        "    loss_val, acc_val = [], []\n",
        "    epochs_array = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                epochs_array.append(epoch)\n",
        "                scheduler.step()\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                loss_train.append(epoch_loss)\n",
        "                acc_train.append(epoch_acc.item())\n",
        "            else:\n",
        "                loss_val.append(epoch_loss)\n",
        "                acc_val.append(epoch_acc.item())\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, model_name))\n",
        "                epochs_no_improve = 0\n",
        "                best_acc = epoch_acc\n",
        "            elif phase == 'val' and epoch_acc <= best_acc:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= max_epochs_stop:\n",
        "                    print(\"Early Stopping!\")\n",
        "                    model.load_state_dict(torch.load(os.path.join(root_dir, model_name)))\n",
        "                    time_elapsed = time.time() - since\n",
        "                    return best_acc, epochs_array, acc_val, acc_train, loss_val, loss_train, time_elapsed\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    return best_acc, epochs_array, acc_val, acc_train, loss_val, loss_train, time_elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_6MVw5HgDJY"
      },
      "outputs": [],
      "source": [
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues, save_name='confusion_matrix.png'):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_name)\n",
        "    plt.clf()\n",
        "    plt.close()\n",
        "\n",
        "# Function to plot ROC curve\n",
        "def roc_plotter(fpr, tpr, auc_score, savename):\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic on Test set')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(savename)\n",
        "    plt.clf()\n",
        "    plt.close()\n",
        "\n",
        "# Function to plot loss and accuracy over epochs\n",
        "def loss_acc_over_epocs_plotter(epochs_array, m_val, m_train, metric, save_name):\n",
        "    plt.figure()\n",
        "    plt.plot(epochs_array, m_val, label='Validation ' + metric)\n",
        "    plt.plot(epochs_array, m_train, label='Training ' + metric)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric)\n",
        "    plt.title(f'{metric} over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(save_name)\n",
        "    plt.clf()\n",
        "    plt.close()\n",
        "\n",
        "def test_model(model):\n",
        "    print(\"Testing...\")\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "    for inputs, labels in dataloaders['test']:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    acc_test = running_corrects.double() / dataset_sizes['test']\n",
        "    print(f'Test Acc: {acc_test:.4f}')\n",
        "    return acc_test\n",
        "\n",
        "def confusion_matrix_roc_curve_calc(model, cm_save_name='Confusion_Matrix.png', roc_save_name='ROC.png'):\n",
        "    print(\"Calculating confusion matrix and ROC curve...\")\n",
        "    true_labels, test_predictions = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloaders['test']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, test_pred = torch.max(outputs, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            test_predictions.extend(test_pred.cpu().numpy())\n",
        "\n",
        "    CM = confusion_matrix(true_labels, test_predictions)\n",
        "    plot_confusion_matrix(cm=CM, classes=class_names, title='Confusion Matrix', save_name=cm_save_name)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(true_labels, test_predictions)\n",
        "    auc_score = roc_auc_score(true_labels, test_predictions)\n",
        "    roc_plotter(fpr, tpr, auc_score, roc_save_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcSjflSLgGcz",
        "outputId": "d2a0410a-d2c2-4234-bfbf-64ec6cacd8d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finetuning the convnet...\n",
            "Epoch 0/9\n",
            "train Loss: 0.1652 Acc: 0.9383\n",
            "val Loss: 0.7518 Acc: 0.7500\n",
            "Epoch 1/9\n",
            "train Loss: 0.0921 Acc: 0.9693\n",
            "val Loss: 0.4302 Acc: 0.8750\n",
            "Epoch 2/9\n",
            "train Loss: 0.0711 Acc: 0.9801\n",
            "val Loss: 0.4851 Acc: 0.8125\n",
            "Epoch 3/9\n",
            "train Loss: 0.0522 Acc: 0.9833\n",
            "val Loss: 0.3520 Acc: 0.8750\n",
            "Epoch 4/9\n",
            "train Loss: 0.0444 Acc: 0.9845\n",
            "val Loss: 0.2346 Acc: 0.9375\n",
            "Epoch 5/9\n",
            "train Loss: 0.0446 Acc: 0.9856\n",
            "val Loss: 0.0692 Acc: 0.9375\n",
            "Epoch 6/9\n",
            "train Loss: 0.0122 Acc: 0.9964\n",
            "val Loss: 0.0243 Acc: 1.0000\n",
            "Epoch 7/9\n",
            "train Loss: 0.0077 Acc: 0.9975\n",
            "val Loss: 0.0507 Acc: 0.9375\n",
            "Epoch 8/9\n",
            "train Loss: 0.0088 Acc: 0.9979\n",
            "val Loss: 0.0313 Acc: 1.0000\n",
            "Epoch 9/9\n",
            "train Loss: 0.0058 Acc: 0.9990\n",
            "val Loss: 0.0987 Acc: 0.9375\n",
            "Training complete in 18m 11s\n",
            "Best validation accuracy: 1.0000\n",
            "Testing...\n",
            "Test Acc: 0.8205\n",
            "Test accuracy: 0.8205\n",
            "Calculating confusion matrix and ROC curve...\n",
            "\n",
            "ConvNet as fixed feature extractor...\n",
            "Epoch 0/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.3503 Acc: 0.8606\n",
            "val Loss: 0.3984 Acc: 0.8750\n",
            "Epoch 1/9\n",
            "train Loss: 0.3002 Acc: 0.8840\n",
            "val Loss: 0.3280 Acc: 0.9375\n",
            "Epoch 2/9\n",
            "train Loss: 0.2956 Acc: 0.8880\n",
            "val Loss: 0.2509 Acc: 0.8750\n",
            "Epoch 3/9\n",
            "train Loss: 0.3232 Acc: 0.8806\n",
            "val Loss: 0.5215 Acc: 0.8125\n",
            "Epoch 4/9\n",
            "train Loss: 0.3447 Acc: 0.8692\n",
            "val Loss: 0.4470 Acc: 0.8750\n",
            "Epoch 5/9\n",
            "train Loss: 0.2877 Acc: 0.8967\n",
            "val Loss: 0.2164 Acc: 0.8750\n",
            "Epoch 6/9\n",
            "train Loss: 0.2338 Acc: 0.9135\n",
            "val Loss: 0.2559 Acc: 0.8750\n",
            "Epoch 7/9\n",
            "train Loss: 0.2351 Acc: 0.9091\n",
            "val Loss: 0.4723 Acc: 0.8125\n",
            "Epoch 8/9\n",
            "train Loss: 0.2159 Acc: 0.9160\n",
            "val Loss: 0.3105 Acc: 0.7500\n",
            "Epoch 9/9\n",
            "train Loss: 0.2179 Acc: 0.9166\n",
            "val Loss: 0.5350 Acc: 0.7500\n",
            "Training complete in 16m 56s\n",
            "Best validation accuracy: 0.9375\n",
            "Testing...\n",
            "Test Acc: 0.8702\n",
            "Test accuracy: 0.8702\n",
            "Calculating confusion matrix and ROC curve...\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs, max_epochs_stop, model_name):\n",
        "    since = time.time()\n",
        "    best_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    loss_train, acc_train = [], []\n",
        "    loss_val, acc_val = [], []\n",
        "    epochs_array = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                epochs_array.append(epoch)\n",
        "                scheduler.step()\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                loss_train.append(epoch_loss)\n",
        "                acc_train.append(epoch_acc.item())\n",
        "            else:\n",
        "                loss_val.append(epoch_loss)\n",
        "                acc_val.append(epoch_acc.item())\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, model_name))\n",
        "                epochs_no_improve = 0\n",
        "                best_acc = epoch_acc\n",
        "            elif phase == 'val' and epoch_acc <= best_acc:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= max_epochs_stop:\n",
        "                    print(\"Early Stopping!\")\n",
        "                    model.load_state_dict(torch.load(os.path.join(root_dir, model_name)))\n",
        "                    time_elapsed = time.time() - since\n",
        "                    return best_acc, epochs_array, acc_val, acc_train, loss_val, loss_train, time_elapsed\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    return best_acc, epochs_array, acc_val, acc_train, loss_val, loss_train, time_elapsed\n",
        "\n",
        "def run_experiments():\n",
        "    # Fine-tuning the convnet\n",
        "    print(\"Finetuning the convnet...\")\n",
        "    model_ft = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=finetune_learning_rate)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=finetune_scheduler_step_size, gamma=finetune_scheduler_gamma)\n",
        "\n",
        "    best_acc_ft, epochs_ft, acc_val_ft, acc_train_ft, loss_val_ft, loss_train_ft, time_elapsed_ft = \\\n",
        "        train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs, max_epochs_stop=max_epochs_stop, model_name=finetune_model_name)\n",
        "\n",
        "    print(f'Training complete in {time_elapsed_ft // 60:.0f}m {time_elapsed_ft % 60:.0f}s')\n",
        "    print(f'Best validation accuracy: {best_acc_ft:.4f}')\n",
        "\n",
        "    loss_acc_over_epocs_plotter(epochs_ft, acc_val_ft, acc_train_ft, 'Accuracy', 'FineTuned_NN_Accuracy_over_epochs.png')\n",
        "    loss_acc_over_epocs_plotter(epochs_ft, loss_val_ft, loss_train_ft, 'Loss', 'FineTuned_NN_Loss_over_epochs.png')\n",
        "\n",
        "    model_ft.load_state_dict(torch.load(os.path.join(root_dir, finetune_model_name)))\n",
        "    test_acc_ft = test_model(model_ft)\n",
        "    print(f'Test accuracy: {test_acc_ft:.4f}')\n",
        "    confusion_matrix_roc_curve_calc(model_ft, 'FineTune_ConfusionMatrix.png', 'FineTuned_ROC.png')\n",
        "\n",
        "    # ConvNet as fixed feature extractor\n",
        "    print('\\nConvNet as fixed feature extractor...')\n",
        "    model_conv = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    for param in model_conv.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    num_ftrs = model_conv.fc.in_features\n",
        "    model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "    model_conv = model_conv.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=fixed_extractor_learning_rate, momentum=fixed_extractor_momentum)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=fixed_extractor_scheduler_step_size, gamma=fixed_extractor_scheduler_gamma)\n",
        "\n",
        "    best_acc_conv, epochs_conv, acc_val_conv, acc_train_conv, loss_val_conv, loss_train_conv, time_elapsed_conv = \\\n",
        "        train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=num_epochs, max_epochs_stop=max_epochs_stop, model_name=fixed_extractor_model_name)\n",
        "\n",
        "    print(f'Training complete in {time_elapsed_conv // 60:.0f}m {time_elapsed_conv % 60:.0f}s')\n",
        "    print(f'Best validation accuracy: {best_acc_conv:.4f}')\n",
        "\n",
        "    loss_acc_over_epocs_plotter(epochs_conv, acc_val_conv, acc_train_conv, 'Accuracy', 'Pretrained_NN_Accuracy_over_epochs.png')\n",
        "    loss_acc_over_epocs_plotter(epochs_conv, loss_val_conv, loss_train_conv, 'Loss', 'Pretrained_NN_Loss_over_epochs.png')\n",
        "\n",
        "    model_conv.load_state_dict(torch.load(os.path.join(root_dir, fixed_extractor_model_name)))\n",
        "    test_acc_conv = test_model(model_conv)\n",
        "    print(f'Test accuracy: {test_acc_conv:.4f}')\n",
        "    confusion_matrix_roc_curve_calc(model_conv, 'Pretrained_ConfusionMatrix.png', 'Pretrained_ROC.png')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdLmpQQtfvMp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
